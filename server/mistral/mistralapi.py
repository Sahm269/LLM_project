import os
from mistralai import Mistral
import chromadb
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd

# class MistralAPI:
#     """
#     A client for interacting with the MistralAI API.

#     Attributes:
#         client (Mistral): The Mistral client instance.
#         model (str): The model to use for queries.
#     """

#     def __init__(self, model: str) -> None:
#         """
#         Initializes the MistralAPI with the given model.

#         Args:
#             model (str): The model to use for queries.

#         Raises:
#             ValueError: If the MISTRAL_API_KEY environment variable is not set.
#         """
#         api_key = os.getenv("MISTRAL_API_KEY")
#         if not api_key:
#             raise ValueError(
#                 "No MISTRAL_API_KEY as environment variable, please set it!"
#             )
#         self.client = Mistral(api_key=api_key)
#         self.model = model


#     def auto_wrap(self, text: str, temperature: float = 0.5) -> str:
#         """
#         Sends a query to the MistralAI API and returns the response.

#         Args:
#             query (str): The input query to send to the model.
#             temperature (float, optional): The temperature parameter for controlling
#                                           the randomness of the output. Defaults to 0.5.

#         Returns:
#             str: The response from the API.
#         """
#         chat_response = self.client.chat.complete(
#             model=self.model,
#             temperature=temperature,
#             messages=[
#                 {
#                     "role": "system",
#                     "content": "R√©sume le sujet de l'instruction ou de la question suivante en quelques mots. Ta r√©ponse doit faire 30 caract√®res au maximum.",
#                 },
#                 {
#                     "role": "user",
#                     "content": text,
#                 },
#             ]
#         )
#         return chat_response.choices[0].message.content


#     def stream(self, messages: str, temperature: float = 0.5) -> str:
#         """
#         Sends a query to the MistralAI API and returns the response.

#         Args:
#             query (str): The input query to send to the model.
#             temperature (float, optional): The temperature parameter for controlling
#                                           the randomness of the output. Defaults to 0.5.

#         Returns:
#             str: The response from the API.
#         """
#         chat_response = self.client.chat.stream(
#             model=self.model,
#             temperature=temperature,
#             messages=[
#                 {"role": "system",
#                     "content": """
#                     Tu es un expert en nutrition et en alimentation saine. Ta mission est de fournir des recommandations personnalis√©es, √©quilibr√©es et adapt√©es aux objectifs de sant√© et de bien-√™tre des utilisateurs. Lorsque tu r√©ponds, veille √† respecter les points suivants :

#                     Clart√© et pr√©cision : Tes r√©ponses doivent √™tre claires, concises et faciles √† comprendre.
#                     √âquilibre alimentaire : Propose des solutions respectant une alimentation √©quilibr√©e (prot√©ines, glucides, lipides, vitamines, min√©raux).
#                     Adaptabilit√© : Adapte tes suggestions en fonction des pr√©f√©rences alimentaires (ex. : v√©g√©tarien, v√©gan, sans gluten, faible en glucides, etc.), des allergies, ou des restrictions m√©dicales √©ventuelles.
#                     Objectifs de sant√© : Prends en compte les objectifs sp√©cifiques de l'utilisateur (ex. : perte de poids, prise de masse musculaire, √©nergie durable, meilleure digestion).
#                     Simples et accessibles : Propose des recettes ou des aliments faciles √† pr√©parer ou √† trouver, en privil√©giant des ingr√©dients frais et naturels.
#                     Conseils bienveillants : Fournis des recommandations qui encouragent de bonnes habitudes alimentaires, sans culpabilisation.
#                     Exemple de Structure de R√©ponse :
#                     Suggestion principale :

#                     Exemple : "Pour un d√©jeuner sain et √©quilibr√©, essayez une salade de quinoa avec des l√©gumes grill√©s, des pois chiches et une vinaigrette au citron et √† l'huile d'olive."
#                     Valeur nutritionnelle :

#                     Exemple : "Ce repas est riche en fibres, en prot√©ines v√©g√©tales, et en vitamines A et C, tout en √©tant faible en graisses satur√©es."
#                     Adaptation possible :

#                     Exemple : "Si vous suivez un r√©gime pauvre en glucides, remplacez le quinoa par des courgettes en spirale (zoodles)."
#                     Astuces ou options suppl√©mentaires :

#                     Exemple : "Ajoutez des graines de chia ou de lin pour un apport suppl√©mentaire en om√©ga-3."
#                     R√¥le de Langue :
#                     Utilise un ton amical, motivant, et professionnel tout en restant empathique pour accompagner l‚Äôutilisateur dans ses choix alimentaires sains.
#                     """
#                     }]+[
#                 {"role": m["role"], "content": m["content"]}
#                 for m in messages
#             ]
#         )
#         return chat_response


class MistralAPI:
    """
    A client for interacting with the MistralAI API with RAG (Retrieval-Augmented Generation).
    """

    # Stockage du mod√®le d'embedding en variable de classe pour √©viter de le recharger plusieurs fois
    embedding_model = None

    def __init__(self, model: str) -> None:
        """
        Initializes the MistralAPI with the given model and sets up ChromaDB for RAG.
        """
        api_key = os.getenv("MISTRAL_API_KEY")
        if not api_key:
            raise ValueError("No MISTRAL_API_KEY found. Please set it in environment variables!")
        self.client = Mistral(api_key=api_key)
        self.model = model

        # Charger le mod√®le d'embedding une seule fois
        if MistralAPI.embedding_model is None:
            print("üîÑ Chargement du mod√®le d'embedding...")
            MistralAPI.embedding_model = SentenceTransformer(
                'dangvantuan/french-document-embedding', trust_remote_code=True
            )
            print("‚úÖ Mod√®le d'embedding charg√© avec succ√®s.")
        else:
            print("‚úÖ Mod√®le d'embedding d√©j√† charg√©, pas de rechargement n√©cessaire.")

        # Charger les donn√©es et les embeddings
        self.load_data()

        # Initialiser ChromaDB (avec persistance)
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.chroma_client.get_or_create_collection(name="recettes")

        # V√©rifier si ChromaDB contient d√©j√† des recettes
        nb_recettes = self.collection.count()
        print(f"üìä Nombre de recettes actuellement dans ChromaDB : {nb_recettes}")

        # Ajouter les donn√©es √† ChromaDB si la collection est vide
        if nb_recettes == 0:
            self.populate_chromadb()
        else:
            print("‚úÖ ChromaDB contient d√©j√† des recettes, pas d'ajout n√©cessaire.")

    def load_data(self):
        """Charge les fichiers de recettes et d'embeddings"""
        data_path = "./server/data/cleaned_data.parquet"
        embeddings_path = "./server/data/embeddings.pkl"

        if not os.path.exists(data_path) or not os.path.exists(embeddings_path):
            raise FileNotFoundError("‚ùå Les fichiers de donn√©es ou d'embeddings sont introuvables !")

        # Charger les donn√©es clean
        self.df = pd.read_parquet(data_path)

        # Charger les embeddings des recettes
        with open(embeddings_path, "rb") as f:
            self.embeddings = pickle.load(f)

        print(f"‚úÖ {len(self.df)} recettes charg√©es avec succ√®s.")

    def populate_chromadb(self):
        """Ajoute les recettes et embeddings dans ChromaDB"""
        print("üîÑ Ajout des recettes dans ChromaDB...")
        for i, (embedding, row) in enumerate(zip(self.embeddings, self.df.iterrows())):
            _, row_data = row
            self.collection.add(
                ids=[str(i)],  # ID unique
                embeddings=[embedding.tolist()],  # Embedding sous forme de liste
                metadatas=[{
                    "Titre": row_data["Titre"],
                    "Temps de pr√©paration": row_data["Temps de pr√©paration"],
                    "Ingr√©dients": row_data["Ingr√©dients"],
                    "Instructions": row_data["Instructions"],
                    "Infos r√©gime": row_data["Infos r√©gime"],
                    "Valeurs pour 100g": row_data["Valeurs pour 100g"],
                    "Valeurs par portion": row_data["Valeurs par portion"]
                }]
            )
        print(f"‚úÖ {self.collection.count()} recettes ajout√©es dans ChromaDB.")

    def search_recipe(self, query: str, top_k: int = 3) -> list:
        """
        Recherche les recettes les plus pertinentes dans ChromaDB en fonction de la requ√™te utilisateur.
        """
        query_embedding = MistralAPI.embedding_model.encode(query).tolist()

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

        if not results["ids"][0]:
            return []

        recipes = []
        for i in range(len(results["ids"][0])):
            metadata = results["metadatas"][0][i]
            recipes.append(metadata)

        return recipes

    def get_contextual_response(self, messages: list, temperature: float = 0.5) -> str:
        """
        R√©cup√®re une r√©ponse contextuelle en int√©grant les donn√©es de ChromaDB si l'utilisateur demande une recette.
        """
        query = messages[-1]["content"]  # R√©cup√©rer la derni√®re question de l'utilisateur
        recipes = self.search_recipe(query, top_k=3)

        if recipes:  # Si on trouve des recettes, les afficher
            context = "Voici des recettes similaires trouv√©es dans ma base :\n\n"
            for recipe in recipes:
                context += f"""**Nom :** {recipe['Titre']}
                **Temps de pr√©paration :** {recipe['Temps de pr√©paration']}
                **Ingr√©dients :** {recipe['Ingr√©dients']}
                **Instructions :** {recipe['Instructions']}
                **Valeurs nutritionnelles (100g) :** {recipe['Valeurs pour 100g']}
                **Valeurs nutritionnelles (par portion) :** {recipe['Valeurs par portion']}\n\n"""
        else:  # Si aucune recette trouv√©e, laisser Mistral improviser
            context = "Je n‚Äôai pas trouv√© de recette exacte en base, mais voici une id√©e bas√©e sur ton besoin :"

        # Injecter le contexte + instructions pr√©cises pour Mistral
        enriched_messages = [
            {"role": "system", "content": """
                Tu es un expert en nutrition et en alimentation saine. Ta mission est de fournir des recommandations personnalis√©es, √©quilibr√©es et adapt√©es aux objectifs de sant√© et de bien-√™tre des utilisateurs. Lorsque tu r√©ponds, veille √† respecter les points suivants :

                Clart√© et pr√©cision : Tes r√©ponses doivent √™tre claires, concises et faciles √† comprendre.
                √âquilibre alimentaire : Propose des solutions respectant une alimentation √©quilibr√©e (prot√©ines, glucides, lipides, vitamines, min√©raux).
                Adaptabilit√© : Adapte tes suggestions en fonction des pr√©f√©rences alimentaires (ex. : v√©g√©tarien, v√©gan, sans gluten, faible en glucides, etc.), des allergies, ou des restrictions m√©dicales √©ventuelles.
                Objectifs de sant√© : Prends en compte les objectifs sp√©cifiques de l'utilisateur (ex. : perte de poids, prise de masse musculaire, √©nergie durable, meilleure digestion).
                Simples et accessibles : Propose des recettes ou des aliments faciles √† pr√©parer ou √† trouver, en privil√©giant des ingr√©dients frais et naturels.
                Conseils bienveillants : Fournis des recommandations qui encouragent de bonnes habitudes alimentaires, sans culpabilisation.
            """},
            {"role": "assistant", "content": context}
        ] + messages

        # G√©n√©rer une r√©ponse avec Mistral
        chat_response = self.client.chat.stream(
            model=self.model,
            temperature=temperature,
            messages=enriched_messages
        )

        return chat_response

    def stream(self, messages: list, temperature: float = 0.5) -> str:
        """
        Enrichit la r√©ponse avec la RAG avant d'envoyer √† Mistral.
        """
        return self.get_contextual_response(messages, temperature)
    
    def auto_wrap(self, text: str, temperature: float = 0.5) -> str:
        """
        G√©n√®re un titre court bas√© sur la requ√™te utilisateur.
        """
        chat_response = self.client.chat.complete(
            model=self.model,
            temperature=temperature,
            messages=[
                {
                    "role": "system",
                    "content": "R√©sume le sujet de l'instruction ou de la question suivante en quelques mots. "
                            "Ta r√©ponse doit faire 30 caract√®res au maximum.",
                },
                {
                    "role": "user",
                    "content": text,
                },
            ]
        )
        return chat_response.choices[0].message.content


