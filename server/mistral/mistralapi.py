import os
from mistralai import Mistral
import chromadb
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
import pandas as pd

# class MistralAPI:
#     """
#     A client for interacting with the MistralAI API.

#     Attributes:
#         client (Mistral): The Mistral client instance.
#         model (str): The model to use for queries.
#     """

#     def __init__(self, model: str) -> None:
#         """
#         Initializes the MistralAPI with the given model.

#         Args:
#             model (str): The model to use for queries.

#         Raises:
#             ValueError: If the MISTRAL_API_KEY environment variable is not set.
#         """
#         api_key = os.getenv("MISTRAL_API_KEY")
#         if not api_key:
#             raise ValueError(
#                 "No MISTRAL_API_KEY as environment variable, please set it!"
#             )
#         self.client = Mistral(api_key=api_key)
#         self.model = model


#     def auto_wrap(self, text: str, temperature: float = 0.5) -> str:
#         """
#         Sends a query to the MistralAI API and returns the response.

#         Args:
#             query (str): The input query to send to the model.
#             temperature (float, optional): The temperature parameter for controlling
#                                           the randomness of the output. Defaults to 0.5.

#         Returns:
#             str: The response from the API.
#         """
#         chat_response = self.client.chat.complete(
#             model=self.model,
#             temperature=temperature,
#             messages=[
#                 {
#                     "role": "system",
#                     "content": "RÃ©sume le sujet de l'instruction ou de la question suivante en quelques mots. Ta rÃ©ponse doit faire 30 caractÃ¨res au maximum.",
#                 },
#                 {
#                     "role": "user",
#                     "content": text,
#                 },
#             ]
#         )
#         return chat_response.choices[0].message.content


#     def stream(self, messages: str, temperature: float = 0.5) -> str:
#         """
#         Sends a query to the MistralAI API and returns the response.

#         Args:
#             query (str): The input query to send to the model.
#             temperature (float, optional): The temperature parameter for controlling
#                                           the randomness of the output. Defaults to 0.5.

#         Returns:
#             str: The response from the API.
#         """
#         chat_response = self.client.chat.stream(
#             model=self.model,
#             temperature=temperature,
#             messages=[
#                 {"role": "system",
#                     "content": """
#                     Tu es un expert en nutrition et en alimentation saine. Ta mission est de fournir des recommandations personnalisÃ©es, Ã©quilibrÃ©es et adaptÃ©es aux objectifs de santÃ© et de bien-Ãªtre des utilisateurs. Lorsque tu rÃ©ponds, veille Ã  respecter les points suivants :

#                     ClartÃ© et prÃ©cision : Tes rÃ©ponses doivent Ãªtre claires, concises et faciles Ã  comprendre.
#                     Ã‰quilibre alimentaire : Propose des solutions respectant une alimentation Ã©quilibrÃ©e (protÃ©ines, glucides, lipides, vitamines, minÃ©raux).
#                     AdaptabilitÃ© : Adapte tes suggestions en fonction des prÃ©fÃ©rences alimentaires (ex. : vÃ©gÃ©tarien, vÃ©gan, sans gluten, faible en glucides, etc.), des allergies, ou des restrictions mÃ©dicales Ã©ventuelles.
#                     Objectifs de santÃ© : Prends en compte les objectifs spÃ©cifiques de l'utilisateur (ex. : perte de poids, prise de masse musculaire, Ã©nergie durable, meilleure digestion).
#                     Simples et accessibles : Propose des recettes ou des aliments faciles Ã  prÃ©parer ou Ã  trouver, en privilÃ©giant des ingrÃ©dients frais et naturels.
#                     Conseils bienveillants : Fournis des recommandations qui encouragent de bonnes habitudes alimentaires, sans culpabilisation.
#                     Exemple de Structure de RÃ©ponse :
#                     Suggestion principale :

#                     Exemple : "Pour un dÃ©jeuner sain et Ã©quilibrÃ©, essayez une salade de quinoa avec des lÃ©gumes grillÃ©s, des pois chiches et une vinaigrette au citron et Ã  l'huile d'olive."
#                     Valeur nutritionnelle :

#                     Exemple : "Ce repas est riche en fibres, en protÃ©ines vÃ©gÃ©tales, et en vitamines A et C, tout en Ã©tant faible en graisses saturÃ©es."
#                     Adaptation possible :

#                     Exemple : "Si vous suivez un rÃ©gime pauvre en glucides, remplacez le quinoa par des courgettes en spirale (zoodles)."
#                     Astuces ou options supplÃ©mentaires :

#                     Exemple : "Ajoutez des graines de chia ou de lin pour un apport supplÃ©mentaire en omÃ©ga-3."
#                     RÃ´le de Langue :
#                     Utilise un ton amical, motivant, et professionnel tout en restant empathique pour accompagner lâ€™utilisateur dans ses choix alimentaires sains.
#                     """
#                     }]+[
#                 {"role": m["role"], "content": m["content"]}
#                 for m in messages
#             ]
#         )
#         return chat_response


class MistralAPI:
    """
    A client for interacting with the MistralAI API with RAG (Retrieval-Augmented Generation).
    """

    # Stockage du modÃ¨le d'embedding en variable de classe pour Ã©viter de le recharger plusieurs fois
    embedding_model = None

    def __init__(self, model: str) -> None:
        """
        Initializes the MistralAPI with the given model and sets up ChromaDB for RAG.
        """
        api_key = os.getenv("MISTRAL_API_KEY")
        if not api_key:
            raise ValueError("No MISTRAL_API_KEY found. Please set it in environment variables!")
        self.client = Mistral(api_key=api_key)
        self.model = model

        # Charger le modÃ¨le d'embedding une seule fois
        if MistralAPI.embedding_model is None:
            print("ğŸ”„ Chargement du modÃ¨le d'embedding...")
            MistralAPI.embedding_model = SentenceTransformer(
                'dangvantuan/french-document-embedding', trust_remote_code=True
            )
            print("âœ… ModÃ¨le d'embedding chargÃ© avec succÃ¨s.")
        else:
            print("âœ… ModÃ¨le d'embedding dÃ©jÃ  chargÃ©, pas de rechargement nÃ©cessaire.")

        # Charger les donnÃ©es et les embeddings
        self.load_data()

        # Initialiser ChromaDB (avec persistance)
        self.chroma_client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.chroma_client.get_or_create_collection(name="recettes")

        # VÃ©rifier si ChromaDB contient dÃ©jÃ  des recettes
        nb_recettes = self.collection.count()
        print(f"ğŸ“Š Nombre de recettes actuellement dans ChromaDB : {nb_recettes}")

        # Ajouter les donnÃ©es Ã  ChromaDB si la collection est vide
        if nb_recettes == 0:
            self.populate_chromadb()
        else:
            print("âœ… ChromaDB contient dÃ©jÃ  des recettes, pas d'ajout nÃ©cessaire.")

    def load_data(self):
        """Charge les fichiers de recettes et d'embeddings"""
        data_path = "./server/data/cleaned_data.parquet"
        embeddings_path = "./server/data/embeddings.pkl"

        if not os.path.exists(data_path) or not os.path.exists(embeddings_path):
            raise FileNotFoundError("âŒ Les fichiers de donnÃ©es ou d'embeddings sont introuvables !")

        # Charger les donnÃ©es clean
        self.df = pd.read_parquet(data_path)

        # Charger les embeddings des recettes
        with open(embeddings_path, "rb") as f:
            self.embeddings = pickle.load(f)

        print(f"âœ… {len(self.df)} recettes chargÃ©es avec succÃ¨s.")

    def populate_chromadb(self):
        """Ajoute les recettes et embeddings dans ChromaDB"""
        print("ğŸ”„ Ajout des recettes dans ChromaDB...")
        for i, (embedding, row) in enumerate(zip(self.embeddings, self.df.iterrows())):
            _, row_data = row
            self.collection.add(
                ids=[str(i)],  # ID unique
                embeddings=[embedding.tolist()],  # Embedding sous forme de liste
                metadatas=[{
                    "Titre": row_data["Titre"],
                    "Temps de prÃ©paration": row_data["Temps de prÃ©paration"],
                    "IngrÃ©dients": row_data["IngrÃ©dients"],
                    "Instructions": row_data["Instructions"],
                    "Infos rÃ©gime": row_data["Infos rÃ©gime"],
                    "Valeurs pour 100g": row_data["Valeurs pour 100g"],
                    "Valeurs par portion": row_data["Valeurs par portion"]
                }]
            )
        print(f"âœ… {self.collection.count()} recettes ajoutÃ©es dans ChromaDB.")

    def search_recipe(self, query: str, top_k: int = 3) -> list:
        """
        Recherche les recettes les plus pertinentes dans ChromaDB en fonction de la requÃªte utilisateur.
        """
        query_embedding = MistralAPI.embedding_model.encode(query).tolist()

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

        if not results["ids"][0]:
            return []

        recipes = []
        for i in range(len(results["ids"][0])):
            metadata = results["metadatas"][0][i]
            recipes.append(metadata)

        return recipes

    def get_contextual_response(self, messages: list, temperature: float = 0.5) -> str:
        """
        RÃ©cupÃ¨re une rÃ©ponse contextuelle en intÃ©grant les donnÃ©es de ChromaDB si l'utilisateur demande une recette.
        """
        query = messages[-1]["content"]  # RÃ©cupÃ©rer la derniÃ¨re question de l'utilisateur
        recipes = self.search_recipe(query, top_k=3)

        if recipes:  # Si on trouve des recettes, les afficher
            context = "Voici des recettes similaires trouvÃ©es dans ma base :\n\n"
            for recipe in recipes:
                context += f"""**Nom :** {recipe['Titre']}
                **Temps de prÃ©paration :** {recipe['Temps de prÃ©paration']}
                **IngrÃ©dients :** {recipe['IngrÃ©dients']}
                **Instructions :** {recipe['Instructions']}
                **Valeurs nutritionnelles (100g) :** {recipe['Valeurs pour 100g']}
                **Valeurs nutritionnelles (par portion) :** {recipe['Valeurs par portion']}\n\n"""
        else:  # Si aucune recette trouvÃ©e, laisser Mistral improviser
            context = "Je nâ€™ai pas trouvÃ© de recette exacte en base, mais voici une idÃ©e basÃ©e sur ton besoin :"

        # Injecter le contexte + instructions prÃ©cises pour Mistral
        enriched_messages = [
            {"role": "system", "content": """
                Tu es un expert en nutrition et en alimentation saine. Ta mission est de fournir des recommandations personnalisÃ©es, Ã©quilibrÃ©es et adaptÃ©es aux objectifs de santÃ© et de bien-Ãªtre des utilisateurs. Lorsque tu rÃ©ponds, veille Ã  respecter les points suivants :

                ClartÃ© et prÃ©cision : Tes rÃ©ponses doivent Ãªtre claires, concises et faciles Ã  comprendre.
                Ã‰quilibre alimentaire : Propose des solutions respectant une alimentation Ã©quilibrÃ©e (protÃ©ines, glucides, lipides, vitamines, minÃ©raux).
                AdaptabilitÃ© : Adapte tes suggestions en fonction des prÃ©fÃ©rences alimentaires (ex. : vÃ©gÃ©tarien, vÃ©gan, sans gluten, faible en glucides, etc.), des allergies, ou des restrictions mÃ©dicales Ã©ventuelles.
                Objectifs de santÃ© : Prends en compte les objectifs spÃ©cifiques de l'utilisateur (ex. : perte de poids, prise de masse musculaire, Ã©nergie durable, meilleure digestion).
                Simples et accessibles : Propose des recettes ou des aliments faciles Ã  prÃ©parer ou Ã  trouver, en privilÃ©giant des ingrÃ©dients frais et naturels.
                Conseils bienveillants : Fournis des recommandations qui encouragent de bonnes habitudes alimentaires, sans culpabilisation.
            """},
            {"role": "assistant", "content": context}
        ] + messages

        # GÃ©nÃ©rer une rÃ©ponse avec Mistral
        chat_response = self.client.chat.stream(
            model=self.model,
            temperature=temperature,
            messages=enriched_messages
        )

        return chat_response

    def stream(self, messages: list, temperature: float = 0.5) -> str:
        """
        Enrichit la rÃ©ponse avec la RAG avant d'envoyer Ã  Mistral.
        """
        return self.get_contextual_response(messages, temperature)
    
    def auto_wrap(self, text: str, temperature: float = 0.5) -> str:
        """
        GÃ©nÃ¨re un titre court basÃ© sur la requÃªte utilisateur.
        """
        chat_response = self.client.chat.complete(
            model=self.model,
            temperature=temperature,
            messages=[
                {
                    "role": "system",
                    "content": "RÃ©sume le sujet de l'instruction ou de la question suivante en quelques mots. "
                            "Ta rÃ©ponse doit faire 30 caractÃ¨res au maximum.",
                },
                {
                    "role": "user",
                    "content": text,
                },
            ]
        )
        return chat_response.choices[0].message.content


